# AdaptiveDownsamplingSelector 降采样方法详解

## 一、peakDetectionDownsampling (峰值检测降采样)

`com.yt.server.util.AdaptiveDownsamplingSelector#peakDetectionDownsampling` 是一个基于**特征重要性**排序的降采样算法。

它的核心思想是：**优先保留那些“弯曲程度”最大（即二阶导数绝对值最大）的点**，因为这些点通常对应着波峰、波谷或信号突变的位置，而丢弃那些在直线上或变化平缓的点。

### 核心逻辑步骤

1. **强制保留首尾**：将第一个点和最后一个点的重要性设为最大值（`Double.MAX_VALUE`），确保它们一定被选中，维持数据的时间跨度。
2. **计算重要性（曲率）**：对于中间的每一个点，计算其二阶差分（近似二阶导数）：
   $$ \text{Importance} = | \text{Next}_y - 2 \times \text{Curr}_y + \text{Prev}_y | $$
    * 物理意义：这个值反映了当前点偏离由前后两点构成的直线的程度。如果三点共线，值为0；如果是个尖峰，值会很大。
3. **排序与筛选**：将所有点按重要性从大到小排序，选取前 `targetCount` 个最重要的点。
4. **按序重组**：将选中的点按原始索引（时间顺序）重新排序，输出结果。

### 核心原理解析：什么是二阶导数（曲率）？

简单来说，**二阶导数**描述的是**变化的快慢**，或者说**弯曲的程度**。

在数据处理中，如果说：

* **数值 ($y$)** = 汽车的位置
* **一阶导数 (斜率)** = 汽车的速度（变化的快慢）
* **二阶导数 (曲率)** = 汽车的**加速度/刹车**（速度变化的快慢）

#### 1. 公式解释

在离散的数据点中（比如 $A, B, C$ 三个连续的点），二阶导数通常通过**二阶差分**来计算：

$$ \text{二阶导数} \approx (C - B) - (B - A) = C - 2B + A $$

* $(C - B)$ 是后一段的斜率（后一个变化趋势）
* $(B - A)$ 是前一段的斜率（前一个变化趋势）
* 两者的差值，就是**趋势改变了多少**。

#### 2. 直观举例

假设我们有三个连续的时间点，看看不同的数据形态下，二阶导数（重要性）是多少：

**A. 直线行驶 (平稳/匀速) -> 二阶导数为 0**
数据：`10, 20, 30`

* **直观感受**：这是一条直线，没有弯曲。
* **一阶变化**：
    * 10变到20，涨了 10
    * 20变到30，又涨了 10
    * 速度没变。
* **二阶计算**：
  $$ |30 - 2 \times 20 + 10| = |30 - 40 + 10| = |0| $$
* **结论**：**0**。这三个点在一条直线上，中间那个点 `20` 并不特殊，如果把它删了，直接连 `10` 和 `30`，形状几乎没变。所以它**不重要
  **。

**B. 缓慢加速 (平滑曲线) -> 二阶导数很小**
数据：`10, 12, 16`

* **直观感受**：开始涨得慢，后来涨得快，有一点点弯曲。
* **一阶变化**：
    * 10变到12，涨了 2
    * 12变到16，涨了 4
    * 速度变快了一点点。
* **二阶计算**：
  $$ |16 - 2 \times 12 + 10| = |16 - 24 + 10| = |2| $$
* **结论**：**2**。有一点重要性，但不大。

**C. 急转弯/尖峰 (剧烈突变) -> 二阶导数极大 🔥**
数据：`10, 100, 10`

* **直观感受**：突然冲上去，又突然掉下来。这是一个尖峰（脉冲）。
* **一阶变化**：
    * 10变到100，猛涨 90 (速度极快向上)
    * 100变到10，猛跌 90 (速度极快向下)
    * 方向发生了 180度 大逆转！
* **二阶计算**：
  $$ |10 - 2 \times 100 + 10| = |20 - 200| = |-180| \Rightarrow 180 $$
* **结论**：**180**。数值非常大！中间这个 `100` 是最关键的转折点。如果把它删了，直接连两头的 `10` 和 `10`
  ，整个尖峰就消失了，数据特征就丢失了。所以它**极度重要**。

### 总结

`peakDetectionDownsampling` 算法利用这个原理：

* **数值小**（接近0）：说明是直线或平滑曲线，删了无所谓。
* **数值大**：说明是拐点、尖峰、突变点，必须保留。

这就是为什么它能精准地保留信号中的“特征”而不浪费点数在平直的线段上。

### 举例说明

假设我们有 5 个数据点，代表一个平稳信号中间突然出现一个脉冲，我们要将其降采样为 **3个点**。

**原始数据 (Index: Y值)**：

* **Idx 0**: `10` (起点)
* **Idx 1**: `10` (平稳)
* **Idx 2**: `50` (**突变峰值**)
* **Idx 3**: `10` (平稳)
* **Idx 4**: `10` (终点)

**目标点数**：3

**1. 计算重要性**

* **Idx 0**: 首点 $\rightarrow$ **MAX**
* **Idx 4**: 尾点 $\rightarrow$ **MAX**
* **Idx 1**: $|10(\text{Idx0}) - 2\times10(\text{Idx1}) + 50(\text{Idx2})| = |10 - 20 + 50| = |40| = $ **40**
* **Idx 2**: $|10(\text{Idx1}) - 2\times50(\text{Idx2}) + 10(\text{Idx3})| = |10 - 100 + 10| = |-80| = $ **80** (
  弯曲度最大)
* **Idx 3**: $|50(\text{Idx2}) - 2\times10(\text{Idx3}) + 10(\text{Idx4})| = |50 - 20 + 10| = |40| = $ **40**

**2. 排序结果**

1. **Idx 0** (MAX)
2. **Idx 4** (MAX)
3. **Idx 2** (Score: 80)
4. Idx 1 (Score: 40)
5. Idx 3 (Score: 40)

**3. 筛选 Top 3**
选中的索引集合为：`{0, 4, 2}`

**4. 重组输出**
按索引排序后：`0 -> 2 -> 4`
**最终结果**：保留了起点、峰值点、终点。
`[10, 50, 10]`

### 代码对应

```java
// 1. 初始化首点重要性为无穷大
importances.add(new PointImportance(0, Double.MAX_VALUE));

// 2. 遍历中间点计算二阶差分
        for(
int i = 1; i <data.

size() -1;i++){
double prev = data.get(i - 1).getY().doubleValue();
double curr = data.get(i).getY().doubleValue();
double next = data.get(i + 1).getY().doubleValue();
// 对应公式 |next - 2*curr + prev|
    importances.

add(new PointImportance(i, Math.abs(next-2*curr+prev)));
        }

// 3. 初始化尾点重要性为无穷大
        importances.

add(new PointImportance(data.size() -1,Double.MAX_VALUE));

// 4. 按重要性倒序排序
        importances.

sort((a, b) ->Double.

compare(b.importance, a.importance));

// 5. 取前 targetCount 个点的索引
Set<Integer> selectedIndices = new HashSet<>();
for(
int i = 0; i <Math.

min(targetCount, importances.size());i++){
        selectedIndices.

add(importances.get(i).index);
        }

// 6. 按原始索引顺序重组数据
List<Integer> sortedIndices = new ArrayList<>(selectedIndices);
Collections.

sort(sortedIndices);
```

### 适用场景

这种方法非常适合 **Step（阶跃）** 或 **Pulse（脉冲）** 类型的信号，因为它能极其精准地抓住信号发生突变的关键位置，而不会像普通均匀采样那样可能“漏掉”尖峰。

---

## 二、adaptiveLTTB (自适应LTTB)

`com.yt.server.util.AdaptiveDownsamplingSelector#adaptiveLTTB` 是标准 LTTB (Largest-Triangle-Three-Buckets)
算法的改进版，专门用于处理**非均匀分布**的复杂数据。

它的核心思想是：**按需分配**。即数据变化剧烈（复杂度高）的区域分配更多的采样点，而变化平缓的区域分配较少的点，从而在总点数不变的情况下，最大化保留细节。

### 核心逻辑步骤

1. **数据分段**：将整个数据集切分为多个片段（默认为 10 段），以便进行局部特征分析。
2. **计算复杂度**：对每一段计算其“复杂度”。
    * **计算公式**：该段内所有点垂直变化量（Y轴变化绝对值）的总和 + 1.0（基础分）。
    * **通俗理解**：就是看这段数据在上下跳动了多少次、跳动幅度有多大。跳得越欢，复杂度越高。
3. **动态分配配额**：根据每段复杂度占总复杂度的比例，分配目标点数 (`targetCount`)。
    * **公式**：`SegmentTarget = TotalTarget * (SegmentComplexity / TotalComplexity)`
    * **例如**：如果某一段波动剧烈，占据了总波动量的 50%，那么它将获得 50% 的采样点名额。
4. **局部 LTTB**：对每一段数据，使用分配到的名额执行标准的 LTTB 降采样。
5. **结果合并与去重**：将所有段的采样结果拼接起来。需要特别处理段与段之间的边界点，避免重复添加。

### 举例说明

假设有一段长度为 1000 的数据，目标降采样为 100 点。

**如果是标准 LTTB**：

* 会机械地将前 500 点分成 50 个桶，后 500 点也分成 50 个桶。
* 结果：平缓区域浪费了大量点，而震荡区域因为点数不足而严重失真。

**如果是 adaptiveLTTB**：

1. **分段**：
   我们将数据切分为两段：
    * **第一段（前500点）**：平缓区域，假设平均波动极小（0.1）。
    * **第二段（后500点）**：震荡区域，假设平均波动很大（10.0）。

2. **计算复杂度（关键步骤）**：
   算法会计算每一段的“垂直路径总长度”，作为复杂度的度量。
    * **第一段复杂度** = 500个点 × 平均波动0.1 + 基础分1.0 = **51.0**
    * **第二段复杂度** = 500个点 × 平均波动10.0 + 基础分1.0 = **5001.0**
    * **总复杂度** = 51.0 + 5001.0 = **5052.0**

   *可以看到，虽然两段数据长度一样，但第二段的复杂度是第一段的近100倍。*

3. **分配配额（按需分配）**：
   总共有 **100个** 采样名额，我们按复杂度的比例来瓜分。
    * **第一段能分多少？**
      占比 = 51.0 / 5052.0 ≈ 1%
      名额 = 100 × 1% = **1 个点**
    * **第二段能分多少？**
      占比 = 5001.0 / 5052.0 ≈ 99%
      名额 = 100 × 99% = **99 个点**

4. **执行结果**：
    * 平缓的前半段只用 1 个点概括（因为它本来就没什么变化，1个点足够了）。
    * 剧烈波动的后半段用了 99 个点（保留了极其丰富的细节）。

5. **最终效果**：
   相比标准算法平均分配（各50点），自适应算法将宝贵的采样资源集中到了“刀刃”上，完美还原了后半段的震荡波形，同时没有浪费资源在前半段的直线上。

### 代码对应

```java
// 1. 分段计算复杂度
for(int i = 0;
i<numSegments;i++){
// ... 切分数据 ...
double complexity = calculateSegmentComplexity(segment);
    segmentComplexity.

add(complexity);

totalComplexity +=complexity;
}

// 2. 根据复杂度分配点数并执行 LTTB
        for(
int i = 0;
i<numSegments;i++){
// ...
// 按比例分配点数
int segmentTarget = (int) Math.round(targetCount * segmentComplexity.get(i) / totalComplexity);
segmentTarget =Math.

max(2,segmentTarget); // 至少保留2点

// 执行 LTTB
List<UniPoint> segmentResult = LTThreeBuckets.sorted(segment, segmentTarget);

// ... 合并结果 ...
}
```

### 适用场景

这种方法非常适合**混合型数据**，即数据中同时包含长段的平稳区和局部的剧烈波动区。例如：

* 机器运行日志（长时间待机 + 短时间高负荷工作）
* 金融交易数据（横盘整理 + 剧烈拉升）

---

## 三、calculateNormalizedVolatility (归一化波动率计算)

`com.yt.server.util.AdaptiveDownsamplingSelector#calculateNormalizedVolatility` 用于衡量数据相对于其自身幅度的“抖动”程度，是识别高频噪声与低频信号的关键指标。

### 核心逻辑步骤

1. **定义**：计算序列中相邻点变化的平均相对幅度。通常用于去趋势后的残差序列（Residuals）。
2. **遍历计算**：对于序列中的每一对相邻点 $(y_{i-1}, y_i)$：
    * **计算绝对差值**：$|y_i - y_{i-1}|$ （这次跳了多远）
    * **计算平均振幅（基准）**：$(|y_i| + |y_{i-1}|) / 2$ （这大概是在什么水位上跳的）
    * **计算相对变化率**：差值 / 平均振幅
3. **求平均**：将所有点的相对变化率取平均值，得到最终的归一化波动率。

### 为什么需要归一化？

* **绝对波动率**：如果一个信号从 1000 跳到 1001，差值是 1；另一个信号从 1 跳到 2，差值也是 1。绝对波动率认为它们一样。
* **归一化波动率**：
    * 1000 -> 1001：变化率约 0.1% （微小波动，可能是正常趋势）
    * 1 -> 2：变化率 100% （剧烈波动，可能是噪声或突变）
    * 归一化后，能公平地比较不同量级数据的“活跃程度”。

### 举例说明（详细步骤）

为了理解“残差”和“波动率”的关系，我们通过计算两组对比鲜明的数据来演示。

**假设前提**：在计算波动率之前，通常会先对数据进行**线性回归（Detrending）**，提取出趋势线，
然后用 `原始值 - 趋势值` 得到**残差 (Residuals)**。
`calculateNormalizedVolatility` 实际上是对这个**残差序列**进行计算。

#### 场景 1：数据 B（平滑趋势信号）

假设数据是一条完美的上升直线，这就是我们最想保留的“信号”。

* **原始数据**: `[10, 20, 30, 40]`
    * *肉眼观察*：非常平滑，是一条直线。

* **第一步：提取趋势**
    * 通过算法拟合，发现这就是一条直线 $y = 10x$。
    * **预测值 (Trend)**: `[10, 20, 30, 40]`

* **第二步：计算残差 (Residuals)**
    * 公式：`残差 = 原始值 - 预测值`
    * Idx 0: $10 - 10 = 0$
    * Idx 1: $20 - 20 = 0$
    * Idx 2: $30 - 30 = 0$
    * Idx 3: $40 - 40 = 0$
    * **残差序列**: `[0, 0, 0, 0]`

* **第三步：计算波动率**
    * 序列中全是 0，没有任何跳动。
    * **归一化波动率 = 0.0**

* **结论**：**极低的波动率**意味着原始数据完全符合趋势，**没有噪声**，是一个非常干净的信号。

---

#### 场景 2：数据 A（高频噪声信号）

假设数据在某个均值上下剧烈反复跳动，这就是我们想识别的“噪声”。

* **原始数据**: `[10, 90, 10, 90]`
    * *肉眼观察*：像心电图一样剧烈抖动，没有明显的上升或下降趋势。

* **第一步：提取趋势**
    * 通过算法拟合，这是一条水平线，平均值是 50。
    * **预测值 (Trend)**: `[50, 50, 50, 50]`

* **第二步：计算残差 (Residuals)**
    * 公式：`残差 = 原始值 - 预测值`
    * Idx 0: $10 - 50 = -40$
    * Idx 1: $90 - 50 = +40$
    * Idx 2: $10 - 50 = -40$
    * Idx 3: $90 - 50 = +40$
    * **残差序列**: `[-40, 40, -40, 40]`
    * *注意：残差保留了原始数据剧烈抖动的特征。*

* **第三步：计算波动率**
    * 我们看相邻点的变化。以第一对点 `(-40, 40)` 为例：
    * **变化幅度**: $|40 - (-40)| = 80$ (跳了80这么远！)
    * **平均振幅**: $(|-40| + |40|) / 2 = 40$ (基准水位是40)
    * **相对变化率**: $80 / 40 = 2.0$ (也就是 **200%** 的剧烈变化)
    * 每一对点都在发生这种 200% 的反转。

* **结论**：**极高的波动率**（远大于0）意味着原始数据相对于趋势线在疯狂跳动，这就是典型的**高频噪声**。

### 总结

* **为什么数据 B 残差为 0？** 因为它完美符合线性趋势，没有任何“意外”的抖动。
* **为什么数据 A 波动率高？** 因为它虽然平均值稳定，但每一刻都在剧烈偏离平均值（残差很大且正负交替），导致计算出的相对变化率极高。

### 应用场景

* **噪声识别**：如果归一化波动率很高，说明数据充满了杂乱的随机跳动，适合使用平滑算法或 Min-Max 包络算法。
* **信号分类**：辅助区分“平稳信号”、“趋势信号”和“剧烈震荡信号”，从而选择最合适的降采样策略。
