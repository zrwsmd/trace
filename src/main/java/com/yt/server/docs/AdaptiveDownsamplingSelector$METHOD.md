# AdaptiveDownsamplingSelector 降采样方法详解

## 一、peakDetectionDownsampling (峰值检测降采样)

`com.yt.server.util.AdaptiveDownsamplingSelector#peakDetectionDownsampling` 是一个基于**特征重要性**排序的降采样算法。

它的核心思想是：**优先保留那些“弯曲程度”最大（即二阶导数绝对值最大）的点**，因为这些点通常对应着波峰、波谷或信号突变的位置，而丢弃那些在直线上或变化平缓的点。

### 核心逻辑步骤

1. **强制保留首尾**：将第一个点和最后一个点的重要性设为最大值（`Double.MAX_VALUE`），确保它们一定被选中，维持数据的时间跨度。
2. **计算重要性（曲率）**：对于中间的每一个点，计算其二阶差分（近似二阶导数）：
   $$ \text{Importance} = | \text{Next}_y - 2 \times \text{Curr}_y + \text{Prev}_y | $$
    * 物理意义：这个值反映了当前点偏离由前后两点构成的直线的程度。如果三点共线，值为0；如果是个尖峰，值会很大。
3. **排序与筛选**：将所有点按重要性从大到小排序，选取前 `targetCount` 个最重要的点。
4. **按序重组**：将选中的点按原始索引（时间顺序）重新排序，输出结果。

### 核心原理解析：什么是二阶导数（曲率）？

简单来说，**二阶导数**描述的是**变化的快慢**，或者说**弯曲的程度**。

在数据处理中，如果说：

* **数值 ($y$)** = 汽车的位置
* **一阶导数 (斜率)** = 汽车的速度（变化的快慢）
* **二阶导数 (曲率)** = 汽车的**加速度/刹车**（速度变化的快慢）

#### 1. 公式解释

在离散的数据点中（比如 $A, B, C$ 三个连续的点），二阶导数通常通过**二阶差分**来计算：

$$ \text{二阶导数} \approx (C - B) - (B - A) = C - 2B + A $$

* $(C - B)$ 是后一段的斜率（后一个变化趋势）
* $(B - A)$ 是前一段的斜率（前一个变化趋势）
* 两者的差值，就是**趋势改变了多少**。

#### 2. 直观举例

假设我们有三个连续的时间点，看看不同的数据形态下，二阶导数（重要性）是多少：

**A. 直线行驶 (平稳/匀速) -> 二阶导数为 0**
数据：`10, 20, 30`

* **直观感受**：这是一条直线，没有弯曲。
* **一阶变化**：
    * 10变到20，涨了 10
    * 20变到30，又涨了 10
    * 速度没变。
* **二阶计算**：
  $$ |30 - 2 \times 20 + 10| = |30 - 40 + 10| = |0| $$
* **结论**：**0**。这三个点在一条直线上，中间那个点 `20` 并不特殊，如果把它删了，直接连 `10` 和 `30`，形状几乎没变。所以它**不重要
  **。

**B. 缓慢加速 (平滑曲线) -> 二阶导数很小**
数据：`10, 12, 16`

* **直观感受**：开始涨得慢，后来涨得快，有一点点弯曲。
* **一阶变化**：
    * 10变到12，涨了 2
    * 12变到16，涨了 4
    * 速度变快了一点点。
* **二阶计算**：
  $$ |16 - 2 \times 12 + 10| = |16 - 24 + 10| = |2| $$
* **结论**：**2**。有一点重要性，但不大。

**C. 急转弯/尖峰 (剧烈突变) -> 二阶导数极大 🔥**
数据：`10, 100, 10`

* **直观感受**：突然冲上去，又突然掉下来。这是一个尖峰（脉冲）。
* **一阶变化**：
    * 10变到100，猛涨 90 (速度极快向上)
    * 100变到10，猛跌 90 (速度极快向下)
    * 方向发生了 180度 大逆转！
* **二阶计算**：
  $$ |10 - 2 \times 100 + 10| = |20 - 200| = |-180| \Rightarrow 180 $$
* **结论**：**180**。数值非常大！中间这个 `100` 是最关键的转折点。如果把它删了，直接连两头的 `10` 和 `10`
  ，整个尖峰就消失了，数据特征就丢失了。所以它**极度重要**。

### 总结

`peakDetectionDownsampling` 算法利用这个原理：

* **数值小**（接近0）：说明是直线或平滑曲线，删了无所谓。
* **数值大**：说明是拐点、尖峰、突变点，必须保留。

这就是为什么它能精准地保留信号中的“特征”而不浪费点数在平直的线段上。

### 举例说明

假设我们有 5 个数据点，代表一个平稳信号中间突然出现一个脉冲，我们要将其降采样为 **3个点**。

**原始数据 (Index: Y值)**：

* **Idx 0**: `10` (起点)
* **Idx 1**: `10` (平稳)
* **Idx 2**: `50` (**突变峰值**)
* **Idx 3**: `10` (平稳)
* **Idx 4**: `10` (终点)

**目标点数**：3

**1. 计算重要性**

* **Idx 0**: 首点 $\rightarrow$ **MAX**
* **Idx 4**: 尾点 $\rightarrow$ **MAX**
* **Idx 1**: $|10(\text{Idx0}) - 2\times10(\text{Idx1}) + 50(\text{Idx2})| = |10 - 20 + 50| = |40| = $ **40**
* **Idx 2**: $|10(\text{Idx1}) - 2\times50(\text{Idx2}) + 10(\text{Idx3})| = |10 - 100 + 10| = |-80| = $ **80** (
  弯曲度最大)
* **Idx 3**: $|50(\text{Idx2}) - 2\times10(\text{Idx3}) + 10(\text{Idx4})| = |50 - 20 + 10| = |40| = $ **40**

**2. 排序结果**

1. **Idx 0** (MAX)
2. **Idx 4** (MAX)
3. **Idx 2** (Score: 80)
4. Idx 1 (Score: 40)
5. Idx 3 (Score: 40)

**3. 筛选 Top 3**
选中的索引集合为：`{0, 4, 2}`

**4. 重组输出**
按索引排序后：`0 -> 2 -> 4`
**最终结果**：保留了起点、峰值点、终点。
`[10, 50, 10]`

### 代码对应

```java
// 1. 初始化首点重要性为无穷大
importances.add(new PointImportance(0, Double.MAX_VALUE));

// 2. 遍历中间点计算二阶差分
        for(
int i = 1; i <data.

size() -1;i++){
double prev = data.get(i - 1).getY().doubleValue();
double curr = data.get(i).getY().doubleValue();
double next = data.get(i + 1).getY().doubleValue();
// 对应公式 |next - 2*curr + prev|
    importances.

add(new PointImportance(i, Math.abs(next-2*curr+prev)));
        }

// 3. 初始化尾点重要性为无穷大
        importances.

add(new PointImportance(data.size() -1,Double.MAX_VALUE));

// 4. 按重要性倒序排序
        importances.

sort((a, b) ->Double.

compare(b.importance, a.importance));

// 5. 取前 targetCount 个点的索引
Set<Integer> selectedIndices = new HashSet<>();
for(
int i = 0; i <Math.

min(targetCount, importances.size());i++){
        selectedIndices.

add(importances.get(i).index);
        }

// 6. 按原始索引顺序重组数据
List<Integer> sortedIndices = new ArrayList<>(selectedIndices);
Collections.

sort(sortedIndices);
```

### 适用场景

这种方法非常适合 **Step（阶跃）** 或 **Pulse（脉冲）** 类型的信号，因为它能极其精准地抓住信号发生突变的关键位置，而不会像普通均匀采样那样可能“漏掉”尖峰。

---

## 二、adaptiveLTTB (自适应LTTB)

`com.yt.server.util.AdaptiveDownsamplingSelector#adaptiveLTTB` 是标准 LTTB (Largest-Triangle-Three-Buckets)
算法的改进版，专门用于处理**非均匀分布**的复杂数据。

它的核心思想是：**按需分配**。即数据变化剧烈（复杂度高）的区域分配更多的采样点，而变化平缓的区域分配较少的点，从而在总点数不变的情况下，最大化保留细节。

### 核心逻辑步骤

1. **数据分段**：将整个数据集切分为多个片段（默认为 10 段），以便进行局部特征分析。
2. **计算复杂度**：对每一段计算其“复杂度”，公式通常为该段内所有点垂直变化量的总和（总路径长）。
   $$ \text{Complexity} = \sum | y_{i} - y_{i-1} | + 1.0 $$
3. **动态分配配额**：根据每段复杂度占总复杂度的比例，分配目标点数 (`targetCount`)。
    * 公式：`SegmentTarget = TotalTarget * (SegmentComplexity / TotalComplexity)`
    * 例如：如果某一段波动剧烈，占据了总波动量的 50%，那么它将获得 50% 的采样点名额。
4. **局部 LTTB**：对每一段数据，使用分配到的名额执行标准的 LTTB 降采样。
5. **结果合并与去重**：将所有段的采样结果拼接起来。需要特别处理段与段之间的边界点，避免重复添加。

### 举例说明

假设有一段长度为 1000 的数据，目标降采样为 100 点。

* **前 500 点**：几乎是一条直线（平缓），复杂度极低。
* **后 500 点**：剧烈震荡的正弦波（复杂），复杂度极高。

**如果是标准 LTTB**：

* 会机械地将前 500 点分成 50 个桶，后 500 点也分成 50 个桶。
* 结果：平缓区域浪费了大量点，而震荡区域因为点数不足而严重失真。

**如果是 adaptiveLTTB**：

1. **分段**：
   我们将数据切分为两段：
    * **第一段（前500点）**：平缓区域，假设平均波动极小（0.1）。
    * **第二段（后500点）**：震荡区域，假设平均波动很大（10.0）。

2. **计算复杂度（关键步骤）**：
   算法会计算每一段的“垂直路径总长度”，作为复杂度的度量。
    * **第一段复杂度** = 500个点 × 平均波动0.1 + 基础分1.0 = **51.0**
    * **第二段复杂度** = 500个点 × 平均波动10.0 + 基础分1.0 = **5001.0**
    * **总复杂度** = 51.0 + 5001.0 = **5052.0**

   *可以看到，虽然两段数据长度一样，但第二段的复杂度是第一段的近100倍。*

3. **分配配额（按需分配）**：
   总共有 **100个** 采样名额，我们按复杂度的比例来瓜分。
    * **第一段能分多少？**
      占比 = 51.0 / 5052.0 ≈ 1%
      名额 = 100 × 1% = **1 个点**
    * **第二段能分多少？**
      占比 = 5001.0 / 5052.0 ≈ 99%
      名额 = 100 × 99% = **99 个点**

4. **执行结果**：
    * 平缓的前半段只用 1 个点概括（因为它本来就没什么变化，1个点足够了）。
    * 剧烈波动的后半段用了 99 个点（保留了极其丰富的细节）。

5. **最终效果**：
   相比标准算法平均分配（各50点），自适应算法将宝贵的采样资源集中到了“刀刃”上，完美还原了后半段的震荡波形，同时没有浪费资源在前半段的直线上。

### 代码对应

```java
// 1. 分段计算复杂度
for(int i = 0;
i<numSegments;i++){
// ... 切分数据 ...
double complexity = calculateSegmentComplexity(segment);
    segmentComplexity.

add(complexity);

totalComplexity +=complexity;
}

// 2. 根据复杂度分配点数并执行 LTTB
        for(
int i = 0;
i<numSegments;i++){
// ...
// 按比例分配点数
int segmentTarget = (int) Math.round(targetCount * segmentComplexity.get(i) / totalComplexity);
segmentTarget =Math.

max(2,segmentTarget); // 至少保留2点

// 执行 LTTB
List<UniPoint> segmentResult = LTThreeBuckets.sorted(segment, segmentTarget);

// ... 合并结果 ...
}
```

### 适用场景

这种方法非常适合**混合型数据**，即数据中同时包含长段的平稳区和局部的剧烈波动区。例如：

* 机器运行日志（长时间待机 + 短时间高负荷工作）
* 金融交易数据（横盘整理 + 剧烈拉升）
